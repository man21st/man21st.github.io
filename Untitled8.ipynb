{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM9nr3MJ5JM5gZyHlgNmUm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/man21st/man21st.github.io/blob/main/Untitled8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCStTSnXAiJv",
        "outputId": "8ca4c97d-1411-4a9d-d8df-a4acb5dee7b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow>=2.10.0 in /usr/local/lib/python3.8/dist-packages (2.11.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.22.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (4.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (23.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (23.1.21)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.14.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (3.19.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.4.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.51.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (0.30.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (0.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (15.0.6.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.11.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.2.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (1.6.3)\n",
            "Requirement already satisfied: keras<2.12,>=2.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.11.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (57.4.0)\n",
            "Requirement already satisfied: tensorboard<2.12,>=2.11 in /usr/local/lib/python3.8/dist-packages (from tensorflow>=2.10.0) (2.11.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow>=2.10.0) (0.38.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.25.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.16.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (6.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (4.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (3.14.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow>=2.10.0) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting remotezip\n",
            "  Downloading remotezip-0.12.1.tar.gz (7.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (4.64.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from remotezip) (2.25.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.8/dist-packages (from remotezip) (0.8.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from opencv-python) (1.22.4)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->remotezip) (2022.12.7)\n",
            "Building wheels for collected packages: remotezip\n",
            "  Building wheel for remotezip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for remotezip: filename=remotezip-0.12.1-py3-none-any.whl size=7947 sha256=486eaf4db464557ffaa4525bb48af9467f51baf77b215d5bbc581f1ae17926e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/69/50/7b5a7fd4fda1cbb85c080b1c05cbbd2f88ac6a665260910b13\n",
            "Successfully built remotezip\n",
            "Installing collected packages: remotezip\n",
            "Successfully installed remotezip-0.12.1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.1\n",
        "# The way this tutorial uses the `TimeDistributed` layer requires TF>=2.10\n",
        "!pip install -U \"tensorflow>=2.10.0\"\n",
        "\n",
        "!pip install remotezip tqdm opencv-python\n",
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "\n",
        "\n",
        "\n",
        "import tqdm\n",
        "import random\n",
        "import pathlib\n",
        "import itertools\n",
        "import collections\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import remotezip as rz\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Some modules to display an animation using imageio.\n",
        "import imageio\n",
        "from IPython import display\n",
        "from urllib import request\n",
        "from tensorflow_docs.vis import embed\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n"
      ],
      "metadata": {
        "id": "hp0R4eNdA1f7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_files_from_zip_url(zip_url):\n",
        "  \"\"\" List the files in each class of the dataset given a URL with the zip file.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL from which the files can be extracted from.\n",
        "\n",
        "    Returns:\n",
        "      List of files in each of the classes.\n",
        "  \"\"\"\n",
        "  files = []\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for zip_info in zip.infolist():\n",
        "      files.append(zip_info.filename)\n",
        "  return files"
      ],
      "metadata": {
        "id": "LkZqB9dECtZD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = list_files_from_zip_url(URL)\n",
        "files = [f for f in files if f.endswith('.avi')]\n",
        "files[:10]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VilvwuwQC1t-",
        "outputId": "b57e61cb-a3d6-4e59-998c-434000206f34"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['UCF101/v_ApplyEyeMakeup_g01_c01.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g01_c02.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g01_c03.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g01_c04.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g01_c05.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g01_c06.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g02_c01.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g02_c02.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g02_c03.avi',\n",
              " 'UCF101/v_ApplyEyeMakeup_g02_c04.avi']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(fname):\n",
        "  \"\"\" Retrieve the name of the class given a filename.\n",
        "\n",
        "    Args:\n",
        "      fname: Name of the file in the UCF101 dataset.\n",
        "\n",
        "    Returns:\n",
        "      Class that the file belongs to.\n",
        "  \"\"\"\n",
        "  return fname.split('_')[-3]\n",
        "\n",
        "def get_files_per_class(files):\n",
        "  \"\"\" Retrieve the files that belong to each class.\n",
        "\n",
        "    Args:\n",
        "      files: List of files in the dataset.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary of class names (key) and files (values). \n",
        "  \"\"\"\n",
        "  files_for_class = collections.defaultdict(list)\n",
        "  print(files_for_class)\n",
        "  for fname in files:\n",
        "    class_name = get_class(fname)\n",
        "    files_for_class[class_name].append(fname)\n",
        "  return files_for_class\n",
        "\n"
      ],
      "metadata": {
        "id": "J2UIH8gNC33h"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 10\n",
        "FILES_PER_CLASS = 50\n",
        "files_for_class = get_files_per_class(files)\n",
        "classes = list(files_for_class.keys())\n",
        "print('Num classes:', len(classes))\n",
        "print('Num videos for class[0]:', len(files_for_class[classes[0]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VND4ffljEZVJ",
        "outputId": "d04c14ea-47f0-4d5e-8149-b6aa7fa93813"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'list'>, {})\n",
            "<class 'collections.defaultdict'>\n",
            "Num classes: 101\n",
            "Num videos for class[0]: 145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def select_subset_of_classes(files_for_class, classes, files_per_class):\n",
        "  \"\"\" Create a dictionary with the class name and a subset of the files in that class.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Dictionary of class names (key) and files (values).\n",
        "      classes: List of classes.\n",
        "      files_per_class: Number of files per class of interest.\n",
        "\n",
        "    Returns:\n",
        "      Dictionary with class as key and list of specified number of video files in that class.\n",
        "  \"\"\"\n",
        "  files_subset = dict()\n",
        "\n",
        "  for class_name in classes:\n",
        "    class_files = files_for_class[class_name]\n",
        "    files_subset[class_name] = class_files[:files_per_class]\n",
        "\n",
        "  return files_subset\n",
        "files_subset = select_subset_of_classes(files_for_class, classes[:NUM_CLASSES], FILES_PER_CLASS)\n",
        "list(files_subset.keys())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpNZYo9vGrO7",
        "outputId": "cd6097cd-f9b7-4293-e3f9-566f037a4921"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ApplyEyeMakeup',\n",
              " 'ApplyLipstick',\n",
              " 'Archery',\n",
              " 'BabyCrawling',\n",
              " 'BalanceBeam',\n",
              " 'BandMarching',\n",
              " 'BaseballPitch',\n",
              " 'BasketballDunk',\n",
              " 'Basketball',\n",
              " 'BenchPress']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_from_zip(zip_url, to_dir, file_names):\n",
        "  \"\"\" Download the contents of the zip file from the zip URL.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL with a zip file containing data.\n",
        "      to_dir: A directory to download data to.\n",
        "      file_names: Names of files to download.\n",
        "  \"\"\"\n",
        "  with rz.RemoteZip(zip_url) as zip:\n",
        "    for fn in tqdm.tqdm(file_names):\n",
        "      class_name = get_class(fn)\n",
        "      zip.extract(fn, str(to_dir / class_name))\n",
        "      unzipped_file = to_dir / class_name / fn\n",
        "\n",
        "      fn = pathlib.Path(fn).parts[-1]\n",
        "      output_file = to_dir / class_name / fn\n",
        "      unzipped_file.rename(output_file)\n",
        "\n"
      ],
      "metadata": {
        "id": "AhYbLolVHhli"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_class_lists(files_for_class, count):\n",
        "  \"\"\" Returns the list of files belonging to a subset of data as well as the remainder of\n",
        "    files that need to be downloaded.\n",
        "\n",
        "    Args:\n",
        "      files_for_class: Files belonging to a particular class of data.\n",
        "      count: Number of files to download.\n",
        "\n",
        "    Returns:\n",
        "      Files belonging to the subset of data and dictionary of the remainder of files that need to be downloaded.\n",
        "  \"\"\"\n",
        "  split_files = []\n",
        "  remainder = {}\n",
        "  for cls in files_for_class:\n",
        "    split_files.extend(files_for_class[cls][:count])\n",
        "    remainder[cls] = files_for_class[cls][count:]\n",
        "  return split_files, remainder\n"
      ],
      "metadata": {
        "id": "yqOjH2F9LKtm"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_ucf_101_subset(zip_url, num_classes, splits, download_dir):\n",
        "  \"\"\" Download a subset of the UCF101 dataset and split them into various parts, such as\n",
        "    training, validation, and test.\n",
        "\n",
        "    Args:\n",
        "      zip_url: A URL with a ZIP file with the data.\n",
        "      num_classes: Number of labels.\n",
        "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
        "              (value is number of files per split).\n",
        "      download_dir: Directory to download data to.\n",
        "\n",
        "    Return:\n",
        "      Mapping of the directories containing the subsections of data.\n",
        "  \"\"\"\n",
        "  files = list_files_from_zip_url(zip_url)\n",
        "  for f in files:\n",
        "    path = os.path.normpath(f)\n",
        "    tokens = path.split(os.sep)\n",
        "    if len(tokens) <= 2:\n",
        "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
        "\n",
        "  files_for_class = get_files_per_class(files)\n",
        "\n",
        "  classes = list(files_for_class.keys())[:num_classes]\n",
        "\n",
        "  for cls in classes:\n",
        "    random.shuffle(files_for_class[cls])\n",
        "\n",
        "  # Only use the number of classes you want in the dictionary\n",
        "  files_for_class = {x: files_for_class[x] for x in classes}\n",
        "\n",
        "  dirs = {}\n",
        "  for split_name, split_count in splits.items():\n",
        "    print(split_name, \":\")\n",
        "    split_dir = download_dir / split_name\n",
        "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
        "    download_from_zip(zip_url, split_dir, split_files)\n",
        "    dirs[split_name] = split_dir\n",
        "\n",
        "  return dirs\n"
      ],
      "metadata": {
        "id": "YJru0K9HIsAl"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "download_dir = pathlib.Path('./UCF101_subset/')\n",
        "subset_paths = download_ucf_101_subset(URL,\n",
        "                                       num_classes = NUM_CLASSES,\n",
        "                                       splits = {\"train\": 30, \"val\": 10, \"test\": 10},\n",
        "                                       download_dir = download_dir)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eljLJYufLLOx",
        "outputId": "ac22718d-f974-44f4-dea9-2909a69a63f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "defaultdict(<class 'list'>, {})\n",
            "train :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [03:40<00:00,  1.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:52<00:00,  1.12s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test :\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 81/100 [01:04<00:12,  1.50it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_count_train = len(list(download_dir.glob('train/*/*.avi')))\n",
        "video_count_val = len(list(download_dir.glob('val/*/*.avi')))\n",
        "video_count_test = len(list(download_dir.glob('test/*/*.avi')))\n",
        "video_total = video_count_train + video_count_val + video_count_test\n",
        "print(f\"Total videos: {video_total}\")\n"
      ],
      "metadata": {
        "id": "YlBQ3umiMObB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_frames(frame, output_size):\n",
        "  \"\"\"\n",
        "    Pad and resize an image from a video.\n",
        "\n",
        "    Args:\n",
        "      frame: Image that needs to resized and padded. \n",
        "      output_size: Pixel size of the output frame image.\n",
        "\n",
        "    Return:\n",
        "      Formatted frame with padding of specified output size.\n",
        "  \"\"\"\n",
        "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
        "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
        "  return frame\n"
      ],
      "metadata": {
        "id": "_jd2l5ycN5ep"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}